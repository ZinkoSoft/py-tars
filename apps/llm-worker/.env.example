# LLM Worker Configuration

# ============================================================================
# MQTT Configuration
# ============================================================================
MQTT_URL=mqtt://user:pass@localhost:1883

# ============================================================================
# LLM Provider Configuration
# ============================================================================
# Provider name (currently only "openai" is supported)
LLM_PROVIDER=openai

# Model name (e.g., gpt-4o, gpt-4-turbo, gpt-3.5-turbo)
LLM_MODEL=gpt-4o

# Maximum tokens per response
LLM_MAX_TOKENS=4096

# Sampling temperature (0.0-2.0, lower = more deterministic)
LLM_TEMPERATURE=0.7

# Nucleus sampling (0.0-1.0)
LLM_TOP_P=1.0

# ============================================================================
# OpenAI Provider Settings
# ============================================================================
# OpenAI API key (required for OpenAI provider)
OPENAI_API_KEY=sk-...

# Optional: Base URL for OpenAI-compatible APIs
# OPENAI_BASE_URL=https://api.openai.com/v1

# Comma-separated list of models using Responses API (supports wildcards)
# Default: gpt-4.1*,gpt-4o-mini*,gpt-5*,gpt-5-mini,gpt-5-nano
OPENAI_RESPONSES_MODELS=gpt-4.1*,gpt-4o-mini*,gpt-5*,gpt-5-mini,gpt-5-nano

# ============================================================================
# RAG Integration (Memory Worker)
# ============================================================================
# Enable RAG retrieval
RAG_ENABLED=false

# Number of documents to retrieve
RAG_TOP_K=5

# Template for injecting RAG context into prompts
# RAG_PROMPT_TEMPLATE="Context: {context}\n\nQuestion: {question}"

# ============================================================================
# Tool Calling (MCP Integration)
# ============================================================================
# Enable tool calling
TOOL_CALLING_ENABLED=false

# Tool registry topic (where tools are announced)
TOPIC_TOOLS_REGISTRY=tools/registry

# Tool call request topic
TOPIC_TOOL_CALL_REQUEST=tools/call/request

# Tool call result topic
TOPIC_TOOL_CALL_RESULT=tools/call/result

# ============================================================================
# Streaming & TTS Integration
# ============================================================================
# Forward LLM stream to TTS
LLM_TTS_STREAM=false

# Minimum characters before flushing to TTS
STREAM_MIN_CHARS=50

# Maximum characters before forced flush
STREAM_MAX_CHARS=200

# Sentence boundary characters
STREAM_BOUNDARY_CHARS=.!?

# ============================================================================
# MQTT Topics
# ============================================================================
# Incoming LLM requests
TOPIC_LLM_REQUEST=llm/request

# Non-streaming responses
TOPIC_LLM_RESPONSE=llm/response

# Streaming deltas
TOPIC_LLM_STREAM=llm/stream

# Cancel streaming requests
TOPIC_LLM_CANCEL=llm/cancel

# Health status (retained)
TOPIC_HEALTH=system/health/llm

# RAG queries to memory-worker
TOPIC_MEMORY_QUERY=memory/query

# RAG results from memory-worker
TOPIC_MEMORY_RESULTS=memory/results

# Current character snapshot (retained)
TOPIC_CHARACTER_CURRENT=system/character/current

# Character update results
TOPIC_CHARACTER_RESULT=character/result

# Request character snapshot
TOPIC_CHARACTER_GET=character/get

# TTS output topic
TOPIC_TTS_SAY=tts/say

# ============================================================================
# Logging
# ============================================================================
# Log level (DEBUG, INFO, WARNING, ERROR, CRITICAL)
LOG_LEVEL=INFO
