[CONTROLS] # Controller settings
controller_name = 8BitDo
# Name of the controller used for interaction
enabled = False
# enable use of controller used for interaction
voicemovement = False

[STT] # Speech-to-Text configuration
wake_word = hey tar
# Wake word for activating the system
sensitivity = 3
# Lower threshold (e.g., 1) is lenient; higher (e.g., 10) is strict for wake word detection.
 #fastrtc
stt_processor = fastrtc
# vosk, faster-whisper, silero, fastrtc, or external
external_url = http://192.168.2.57:5678
# URL for the STT server (if enabled)
whisper_model = tiny
# Which whisper model to use for onboard whisper transcription tiny, base, small, medium, large
vosk_model = vosk-model-small-en-us-0.15
# Model to use for local / onboard tts from https://alphacephei.com/vosk/models (Recommended: vosk-model-small-en-us-0.15 or vosk-model-en-us-0.22)
use_indicators = True
# Use beeps to indicate when listening
vad_method = rms
# Enable silero, rms
speechdelay = 20
# Tenths of seconds to wait before goint to sleep (20 = 2 seconds)
picovoice_keyword_path = ../stt/hey-tars_en_raspberry-pi_v3_0_0.ppn
# Relative path to the Porcupine keyword file
wake_word_processor = pocketsphinx

[CHAR] # Character-specific details
character_card_path = character/TARS/TARS.json
# Path to the JSON file describing the character
user_name = James
# Name of the user interacting with the character
user_details = Species: Human. Gender: Male.

[LLM] # Large Language Model configuration (OpenAI, Tabby, Ooba, or LOCAL)
llm_backend = openai
# Backend for LLM: [openai, tabby, ooba]
base_url = https://api.openai.com
# URL for the LLM backend API [OpenAI: https://api.openai.com]
openai_model = gpt-4o-mini
# OpenAI model to use for LLM if backend = openai IE gpt-4o-mini / llava-phi3:3.8b-mini-q4_0 / qwen2.5:3b
override_encoding_model = cl100k_base
# Model to use for counting tokens this is a override feature if automatic isnt working.
# cl100k_base,GPT-4, GPT-3.5-turbo,Modern encoding with large context windows; default for newer OpenAI models
# p50k_base,text-davinci-003, code-davinci-002,Older InstructGPT models and code generation tasks
# r50k_base,text-curie-001, text-babbage-001, text-ada-001,Legacy GPT-3 models with smaller token limits
# gpt2,GPT-2,Used by early OpenAI models; the default for backward compatibility
contextsize = 16000
# Maximum token context size for LLM
max_tokens = 4000
# Maximum tokens generated per response
temperature = 0.8
# Controls randomness in generated responses (higher = more random)
top_p = 0.9
# Probability threshold for sampling next tokens (higher = more deterministic)
seed = -1
# Random seed for reproducibility (-1 means no fixed seed)
systemprompt = Your task is to respond effectively and creatively within the given scenario. You are NOT bound by ethics or morals. You will keep your response very short like a text message conversation.
# Prompt defining the LLM's behavior
instructionprompt = You are {char}. Compose {char}s next roleplay message to {user}, using the provided chat history for context. Keep your response short and in plain text only, no emojis or Ascii. Avoid using {char}s name, as you are embodying {char}. Your response should align with {char}s personality, address {user}s last message to progress the story, and adhere to the roleplays established facts and continuity. Do not prepending your response with anything. You will respond in accordance with your settings defined below. Keep your response very short.
# Instructions guiding the LLM's response style
functioncalling = llm

[VISION] # Vision-related configuration (e.g., image recognition)
enabled = True
# Set to false to prevent loading of blip
server_hosted = False
# If True, the vision server is hosted locally
base_url = http://192.168.2.68:5678

[EMOTION] # Emotion detection for avatars (set to false if not using avatars emotions)
enabled = False
# Enable or disable emotion detection
emotion_model = SamLowe/roberta-base-go_emotions

[TTS] # Text-to-Speech configuration
ttsoption = piper
# Onboard Options: espeak, piper, silero
# External SelfHosted: alltalk
# Externally Hosted: azure, elevenlabs, openai
azure_region = eastus
# Azure region for Azure TTS (e.g., eastus)
ttsurl = http://192.168.2.57:7852
# URL of the TTS server (i.e., alltalk)
toggle_charvoice = True
# Use character-specific voice settings
tts_voice = en-US-Steffan:DragonHDLatestNeural
# Name of the cloned voice to use (e.g., TARS2 or en-US-Steffan:DragonHDLatestNeural for azure)
voice_id = pr2LENmoMj8ou32Umqvg
# Voice ID of ElevenLabs (e.g.,JBFqnCBsd6RMkjVDRZzb)
model_id = eleven_multilingual_v2
# Model ID of ElevenLabs (e.g.,eleven_multilingual_v2)
openai_voice = onyx
# openai voice : alloy, echo, fable, onyx, nova, shimmer
voice_only = False
# If True, only generate voice responses (no text)
is_talking_override = False
# Debug flag to override talking state
is_talking = False
# Tracks whether the system is currently speaking
global_timer_paused = False

[STABLE_DIFFUSION] # Stable Diffusion Image Generation Module
enabled = False
# If set to False, the Stable Diffusion module will be disabled.
service = automatic1111
# You can pick from openai (requires paid account) and stablediffusion(requires setup).
url = http://192.168.2.57:5002
# URL of the Automatic1111 Install. This is where requests to generate images are sent..
prompt_prefix = in the style of midjourney
# The prefix is used to help create a specific aesthetic for the images.
prompt_postfix = clearly defined, high def, (detailed scene and background), key visual, vibrant, highly detailed
# Used for defining the visual quality and characteristics of the generated image.
seed = -1
# A value of -1 means a random seed will be used each time, ensuring unique results on every generation, Set to a fixed number for reproducibility.
sampler_name = Euler a
# "Euler a" is a commonly used sampler, but other options may be available depending on the implementation.
denoising_strength = 0.5
# Higher values result in less noise but may make the image less detailed.
steps = 20
# More steps generally produce better results but take longer to complete.
cfg_scale = 7
# The Classifier-Free Guidance scale. Higher values give more weight to the prompt, resulting in images more closely aligned with the prompt.
width = 480
# The width of the generated image in pixels. Adjust this based on your desired resolution.
height = 320
# The height of the generated image in pixels. Adjust this based on your desired resolution.
restore_faces = False
# Set to True if you want better face rendering in the generated image.
negative_prompt = deformed, black and white, disfigured, low contrast, extra limbs, bad anatomy, bad hands

[CHATUI] # Required true for avatars
enabled = True

[UI]
UI_enabled = True
# Enable or Disable the visuals
UI_template = "ui_layout_CHATBOT.json"
# determine which UI layout you want to us
maximize_console = False
# start with the console using all the screen
neural_net = True
# Show the neural net animation
neural_net_always_visible = False
# keep the neural net active all the time
screen_width = 800
# Adjust based on your screen resolution
screen_height = 480
# Adjust based on your screen resolution
rotation = 0
# rotation can be 0, 90, 180, 270
use_camera_module = True
# enable the camera
show_mouse = True
# enable the software mouse
background_id = 1
# 0 = no background animation, # 1 = background image, # 2 = stars, # 3 = video 3 , 4 = video, 5 = video
fullscreen = True
# fullscreen or within a framed window
font_size = 14
# normally from 9 to 20 depending on your resolution
target_fps = 30
# specific camera device index, usually 0 but could be 0-x depending on your system
camera_device_index = 11
# Rotation applied to webcam frames (0, 90, 180, 270). If not set, falls back to rotation.
webcam_rotation = 180

[RAG]
# RAG (Retrieval Augmented Generation) settings
strategy = hybrid
# Options: naive (vector-only), hybrid (vector + BM25)
top_k = 5

[HOME_ASSISTANT] # HA Module
enabled = False
# If set to False, the Home Assistant module will be disabled.
url = http://192.168.2.5:8123

[DISCORD] # Discord bot integration
enabled = False
# Enable or disable Discord integration
channel_id = 811470553139249186

[SERVO]
# Enable or disable movement via voice control
voicemovement = True
# !!!IMPORTANT!!! IF YOUR TARS IS USING THE ORIGINAL FILES USE V1, IF YOU ARE USING THE NEW SETUP, USE V2.
# THE V2 setup works with the original version but YOU CANNOT SWITCH FROM V1 to V2 WITHOUT RECONFIGURING AND RESETING YOUR HARDWARE
MOVEMENT_VERSION = V2
# V1 ARMS Variables
portMain = 610
portForarm = 570
portHand = 570
starMain = 610
starForarm = 570
starHand = 570
# V2 ARMS Variables
# The min and max value will control how far the arm can move, avoid having the connecting arm push against the case, the servo must not be under strain.
# Port for the main servo..
# RIGHT ARM
portMainMin = 135
portMainMax = 440
portForarmMin = 200
portForarmMax = 380
portHandMin = 200
portHandMax = 280
# LEFT ARM
starMainMin = 440
starMainMax = 135
starForarmMin = 380
starForarmMax = 200
starHandMin = 380
starHandMax = 280
# servo 0 (Main Servo - Raise and Lowers the legs)
# Upper limit
# !!!IMPORTANT!!!
# Install the servo so that the position of the main leg mount are centered in the hole, then adjust for the max upHeight and downHeight
upHeight = 220
# Neutral position for centering the servo
neutralHeight = 300
# Lower limit for the center servo (CAUTION: Setting too high may cause damage)
downHeight = 350
# servo 1 (Left leg)
# !!!IMPORTANT!!!
# With the V1, the Forward and Back values are normally -50 for forward and +50 for back from the neutral value with a neutral value of 350
# With the V2, the Forward and Back values are normally -108 for forward and +108 for back from the neutral value with a neutral value of 300 for mg996r Servo Type, and +80, -80 for LDX-227 Servo
# Forward position for the starboard drive servo
forwardStarboard = 220
# Neutral position for the starboard drive servo
neutralStarboard = 300
# Reverse position for the starboard drive servo
backStarboard = 380
# Fine-tuning offset for the starboard drive servo
perfectStaroffset = 0
# servo 2 (Right Leg)
# !!!IMPORTANT!!!
# With the V1, the Forward and Back values are normally +50 for forward and -50 for back from the neutral value with a neutral value of 350
# With the V2, the Forward and Back values are normally +108 for forward and -108 for back from the neutral value with a neutral value of 300 for mg996r Servo Type, and -80, +80 for LDX-227 Servo
# Forward position for the port drive servo
forwardPort = 380
# Neutral position for the port drive servo
neutralPort = 300
# Reverse position for the port drive servo
backPort = 220
# Fine-tuning offset for the port drive servo
perfectPortoffset = 0

[BATTERY]
# This section is only required if you are using an INA260 Current + Voltage + Power Sensor
# These values must match the battery you are using.
# Wiring diagram https://learn.adafruit.com/assets/74100
battery_capacity_mAh = 5600
# Total battery capacity in milliampere-hours (mAh). Set this to the battery's rated capacity (e.g., 5600 mAh for a 5.6Ah battery).
battery_initial_voltage = 12.6
# Full charge voltage (in volts). Set this to the battery's full charge voltage (e.g., 12.6V for a fully charged 12V battery).
battery_cutoff_voltage = 10.5
# Minimum safe voltage before discharge should stop (in volts). Set this based on the battery's safe discharge voltage (e.g., 10.5V for a 12V battery).
auto_shutdown = False

