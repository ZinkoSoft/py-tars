# ops/compose.yaml
# Usage:
#   docker compose -f ops/compose.yaml up --build                    # CPU mode (default)
#   docker compose -f ops/compose.yml -f ops/compose.npu.yml up     # NPU mode (RK3588 devices)
#
# NPU Requirements:
#   - RK3588-based device (Orange Pi 5 Max, Rock 5B, etc.)
#   - NPU setup: bash scripts/setup-rknpu.sh
#   - Model conversion: python scripts/convert_tflite_to_rknn.py --input models/openwakeword/hey_tars.tflite --output models/openwakeword/hey_tars.rknn
#
# This file reuses ONE generic Dockerfile (docker/app.Dockerfile) for all apps.
# Each service passes different build args (APP_PATH, APP_MODULE) to build its image.
#
# Repo assumptions (monorepo layout):
#   /apps/router, /apps/tts-worker, /apps/stt-worker
#   /packages/tars-core
#   /docker/app.Dockerfile  (this file references it)
#
# Notes:
# - The apps implement retry-on-connect to MQTT; strict health ordering isn't required.
# - If you provide console scripts in pyproject.toml, you can replace `command:` with them.

name: tars-stack

services:
  mqtt:
    env_file: ../.env
    image: eclipse-mosquitto:2
    container_name: tars-mqtt
    ports:
      - "1883:1883"
    volumes:
      - ./mosquitto.conf:/mosquitto/config/mosquitto.conf:ro
      - ./mosquitto-data:/mosquitto/data
      - ./mosquitto-config:/mosquitto/config
      # volumes:
      #   - ./mosquitto:/mosquitto   # optional: supply custom config/persistence
    restart: unless-stopped
    
  router:
    build:
      context: ..                      # root of the repo
      dockerfile: docker/app.Dockerfile
      args:
        PY_VERSION: "3.11"
        APP_PATH: apps/router
        CONTRACTS_PATH: packages/tars-core
        APP_MODULE: router.__main__
    env_file: ../.env
    image: tars/router:dev
    container_name: tars-router
    environment:
      MQTT_URL: mqtt://mqtt:1883
      MQTT_HOST: mqtt
      MQTT_PORT: "1883"
      LOG_LEVEL: ${LOG_LEVEL:-INFO}
      ROUTER_LLM_TTS_STREAM: ${ROUTER_LLM_TTS_STREAM:-1}
      TOPIC_LLM_STREAM: ${TOPIC_LLM_STREAM:-llm/stream}
      TOPIC_LLM_RESPONSE: ${TOPIC_LLM_RESPONSE:-llm/response}
      TOPIC_LLM_CANCEL: ${TOPIC_LLM_CANCEL:-llm/cancel}
      TOPIC_TTS_SAY: ${TOPIC_TTS_SAY:-tts/say}
      ROUTER_STREAM_MIN_CHARS: ${STREAM_MIN_CHARS:-60}
      ROUTER_STREAM_MAX_CHARS: ${STREAM_MAX_CHARS:-240}
      ROUTER_STREAM_BOUNDARY_CHARS: ${STREAM_BOUNDARY_CHARS:-.!?;:}
      ROUTER_TTS_VOICE: ${ROUTER_TTS_VOICE:-/voices/TARS.onnx}
      PYTHONPATH: /workspace/apps/router
    volumes:
      - ..:/workspace:ro
    depends_on:
      mqtt:
        condition: service_started
    # If router defines a console script `tars-router`, you can use:
    # command: ["tars-router"]
    restart: unless-stopped

  tts:
    build:
      context: ..
      dockerfile: docker/specialized/tts-worker.Dockerfile
    image: tars/tts:dev
    container_name: tars-tts
    env_file: ../.env
    environment:
      MQTT_URL: mqtt://mqtt:1883
      MQTT_HOST: mqtt
      MQTT_PORT: "1883"
      PIPER_VOICE: /voices/TARS.onnx
      PULSE_RUNTIME_PATH: /run/user/1000/pulse
      PULSE_SERVER: unix:/run/user/1000/pulse/native
      TTS_STREAMING: ${TTS_STREAMING:-0}
      TTS_PIPELINE: ${TTS_PIPELINE:-1}
      TTS_WAKE_CACHE_ENABLE: ${TTS_WAKE_CACHE_ENABLE:-1}
      TTS_WAKE_CACHE_DIR: ${TTS_WAKE_CACHE_DIR:-/data/tts-cache}
      TTS_WAKE_CACHE_MAX: ${TTS_WAKE_CACHE_MAX:-16}
      PYTHONPATH: /workspace/apps/tts-worker/src:/workspace/packages/tars-core/src

    depends_on:
      mqtt:
        condition: service_started
    # Image entrypoint handles startup
    restart: unless-stopped
    # Example of mounting voices/models if needed
    volumes:
      - ../apps/tts-worker/voices:/voice-models:ro
      - /run/user/1000/pulse:/run/user/1000/pulse:ro
      - ../data/tts-cache:/data/tts-cache
      - ..:/workspace:ro
      - ./secrets:/etc/tars/secrets:ro

  ui-web:
    build:
      context: ..
      dockerfile: docker/specialized/ui-web.Dockerfile
      args:
        SERVICE_PATH: apps/ui-web
    image: tars/ui-web:dev
    container_name: tars-ui-web
    env_file: ../.env
    environment:
      MQTT_URL: mqtt://mqtt:1883
      MQTT_HOST: mqtt
      MQTT_PORT: "1883"
      HOST: 0.0.0.0
      PORT: "5010"
    ports:
      - "5010:5010"
    depends_on:
      mqtt:
        condition: service_started
    restart: unless-stopped

  camera:
    build:
      context: ..
      dockerfile: docker/specialized/camera-service.Dockerfile
    image: tars/camera:dev
    container_name: tars-camera
    env_file: ../.env
    environment:
      MQTT_URL: mqtt://mqtt:1883
      MQTT_HOST: mqtt
      MQTT_PORT: "1883"
      CAMERA_WIDTH: ${CAMERA_WIDTH:-640}
      CAMERA_HEIGHT: ${CAMERA_HEIGHT:-480}
      CAMERA_FPS: ${CAMERA_FPS:-10}
      CAMERA_QUALITY: ${CAMERA_QUALITY:-80}
      CAMERA_ROTATION: ${CAMERA_ROTATION:-0}
      CAMERA_MQTT_RATE: ${CAMERA_MQTT_RATE:-2}
      CAMERA_HTTP_HOST: 0.0.0.0
      CAMERA_HTTP_PORT: ${CAMERA_HTTP_PORT:-8080}
      LOG_LEVEL: ${LOG_LEVEL:-INFO}
      PYTHONPATH: /workspace/apps/camera-service/src
    command: ["python", "-m", "camera_service"]
    ports:
      - "${CAMERA_HTTP_PORT:-8080}:${CAMERA_HTTP_PORT:-8080}"
    volumes:
      - ..:/workspace:ro
    depends_on:
      mqtt:
        condition: service_started
    restart: unless-stopped
    devices:
      - /dev/video0:/dev/video0
    privileged: true

  ui:
    build:
      context: ..
      dockerfile: docker/specialized/ui.Dockerfile
    image: tars/ui:dev
    container_name: tars-ui
    env_file: ../.env
    network_mode: host  # Use host network for MQTT and X11 simplicity
    ipc: host  # Share IPC namespace for X11
    shm_size: "512m"     # (optional but nice)
    environment:
      MQTT_URL: mqtt://localhost:1883
      MQTT_HOST: localhost
      MQTT_PORT: "1883"
      UI_CONFIG: /config/ui.toml
      DISPLAY: ${DISPLAY:-:0}
      SDL_VIDEODRIVER: ${SDL_VIDEODRIVER:-x11}
      SDL_VIDEO_X11_FORCE_EGL: "1"        # <- force EGL instead of GLX on X11
      SDL_HINT_OPENGL_ES_DRIVER: "1"      # <- hint SDL to prefer OpenGL ES driver
      SDL_OPENGL_ES_DRIVER: "1"           # <- older alias of the same hint
      XAUTHORITY: ${XAUTHORITY:-${HOME}/.Xauthority}
      UI_FFT_WS_ENABLE: "1"
      UI_FFT_WS_URL: ${UI_FFT_WS_URL:-ws://192.168.1.80:8765/fft}
      UI_FFT_WS_RETRY: "5.0"
      # Disable MIT-SHM to avoid attach errors
      SDL_VIDEO_X11_XSHM: "0"
      # OpenGL environment variables
      MESA_LOADER_DRIVER_OVERRIDE: panfrost # <- prefer panfrost on RK3588
      LIBGL_ALWAYS_INDIRECT: "0"
      LIBGL_ALWAYS_SOFTWARE: "0"
      # Force software rendering until GPU driver is fixed
      UI_SOFTWARE_RENDERING: ${UI_SOFTWARE_RENDERING:-0}
      PYOPENGL_PLATFORM: egl # <- use EGL backend for PyOpenGL
      SDL_AUDIODRIVER: dummy
    depends_on:
      mqtt:
        condition: service_started
    volumes:
      - ../apps/ui/ui.toml:/config/ui.toml:ro
      - /tmp/.X11-unix:/tmp/.X11-unix:ro
      - ${HOME}/.Xauthority:${HOME}/.Xauthority:ro
      # GPU/DRI access for hardware-accelerated OpenGL
      - /dev/dri:/dev/dri:rw
    devices:
      # Grant access to GPU devices for OpenGL acceleration
      - /dev/dri:/dev/dri
    group_add:
      - "44"   # 44 on your host
      - "992"  # 992 on your host
    restart: unless-stopped

  llm:
    build:
      context: ..
      dockerfile: docker/specialized/llm-worker.Dockerfile
    image: tars/llm:dev
    container_name: tars-llm
    env_file: ../.env
    environment:
      MQTT_URL: mqtt://${MQTT_USER}:${MQTT_PASS}@mqtt:1883
      MQTT_HOST: mqtt
      MQTT_PORT: "1883"
      LOG_LEVEL: ${LOG_LEVEL:-INFO}
      LLM_PROVIDER: ${LLM_PROVIDER:-openai}
      LLM_MODEL: ${LLM_MODEL:-gpt-4o-mini}
      LLM_MAX_TOKENS: ${LLM_MAX_TOKENS:-512}
      LLM_TEMPERATURE: ${LLM_TEMPERATURE:-0.7}
      LLM_LOG_LEVEL: ${LLM_LOG_LEVEL:-INFO}
      TOOL_CALLING_ENABLED: ${TOOL_CALLING_ENABLED:-false}
      OPENAI_RESPONSES_MODELS: ${OPENAI_RESPONSES_MODELS:-gpt-5*,gpt-5-mini,gpt-5-nano}
      TOPIC_TOOLS_REGISTRY: ${TOPIC_TOOLS_REGISTRY:-llm/tools/registry}
      TOPIC_TOOL_CALL_RESULT: ${TOPIC_TOOL_CALL_RESULT:-llm/tools/result}
      PYTHONPATH: /workspace/apps/llm-worker/src:/workspace/packages/tars-core/src
    command: ["python", "-m", "llm_worker"]
    volumes:
      - ..:/workspace:ro
    depends_on:
      mqtt:
        condition: service_started
    restart: unless-stopped

  memory:
    build:
      context: ..
      dockerfile: docker/specialized/memory-worker.Dockerfile
      args:
        NPU_EMBEDDER_ENABLED: ${NPU_EMBEDDER_ENABLED:-0}
    image: tars/memory:dev
    container_name: tars-memory
    env_file: ../.env
    environment:
      MQTT_URL: mqtt://mqtt:1883
      MQTT_HOST: mqtt
      MQTT_PORT: "1883"
      LOG_LEVEL: ${LOG_LEVEL:-INFO}
      MEMORY_DIR: /data
      MEMORY_FILE: memory.pickle.gz
      CHARACTER_NAME: ${CHARACTER_NAME:-TARS}
      CHARACTER_DIR: /config/characters
      EMBED_MODEL: ${EMBED_MODEL:-sentence-transformers/all-MiniLM-L6-v2}
      RAG_STRATEGY: ${RAG_STRATEGY:-hybrid}
      MEMORY_TOP_K: ${MEMORY_TOP_K:-5}
      # NPU embedder settings (when NPU_EMBEDDER_ENABLED=1)
      NPU_EMBEDDER_ENABLED: ${NPU_EMBEDDER_ENABLED:-0}
      RKNN_EMBEDDER_PATH: ${RKNN_EMBEDDER_PATH:-/data/model_cache/embedder/all-MiniLM-L6-v2.rknn}
      NPU_CORE_MASK: ${NPU_CORE_MASK:-0}
      NPU_FALLBACK_CPU: ${NPU_FALLBACK_CPU:-1}
      PYTHONPATH: /workspace/apps/memory-worker/src:/workspace/packages/tars-core/src
    volumes:
      - ../data/memory:/data
      - ../data/model_cache:/data/model_cache
      - ../apps/memory-worker/characters:/config/characters:ro
      - ..:/workspace:ro
      - /proc/device-tree/compatible:/proc/device-tree/compatible:ro
      - /lib/librknnrt.so:/usr/lib/aarch64-linux-gnu/librknnrt.so:ro
    devices:
      - /dev/dri/renderD129:/dev/dri/renderD129
    depends_on:
      mqtt:
        condition: service_started
    restart: unless-stopped

  movement:
    build:
      context: ..
      dockerfile: docker/app.Dockerfile
      args:
        PY_VERSION: "3.11"
        APP_PATH: apps/movement-service
        CONTRACTS_PATH: packages/tars-core
        APP_MODULE: movement_service
    image: tars/movement:dev
    container_name: tars-movement
    env_file: ../.env
    environment:
      MQTT_URL: mqtt://mqtt:1883
      MQTT_HOST: mqtt
      MQTT_PORT: "1883"
      LOG_LEVEL: ${LOG_LEVEL:-INFO}
      MOVEMENT_TEST_TOPIC: ${MOVEMENT_TEST_TOPIC:-movement/test}
      MOVEMENT_HEALTH_TOPIC: ${MOVEMENT_HEALTH_TOPIC:-system/health/movement}
      MOVEMENT_PUBLISH_QOS: ${MOVEMENT_PUBLISH_QOS:-1}
      PYTHONPATH: /workspace/apps/movement-service/src
    volumes:
      - ..:/workspace:ro
    depends_on:
      mqtt:
        condition: service_started
    restart: unless-stopped

  config-manager:
    build:
      context: ..
      dockerfile: docker/specialized/config-manager.Dockerfile
    image: tars/config-manager:dev
    container_name: tars-config-manager
    env_file: ../.env
    environment:
      MQTT_URL: mqtt://mqtt:1883
      MQTT_HOST: mqtt
      MQTT_PORT: "1883"
      CONFIG_DB_PATH: /data/config/config.db
      CONFIG_LKG_CACHE_PATH: /data/config/config.lkg.json
      CONFIG_EPOCH_PATH: /data/config/health+epoch.json
      CONFIG_MANAGER_HOST: 0.0.0.0
      CONFIG_MANAGER_PORT: "8081"
      CONFIG_MANAGER_LOG_LEVEL: ${LOG_LEVEL:-INFO}
      PYTHONPATH: /workspace/apps/config-manager/src:/workspace/packages/tars-core/src
    ports:
      - "8081:8081"
    volumes:
      - ..:/workspace:ro
      - config-db:/data/config
      - ./secrets:/etc/tars/secrets:ro
    depends_on:
      mqtt:
        condition: service_started
    restart: unless-stopped

  litestream:
    image: litestream/litestream:latest
    container_name: tars-litestream
    env_file: ../.env
    volumes:
      - config-db:/data/config
      - ./litestream.yml:/etc/litestream.yml:ro
    command: replicate
    depends_on:
      config-manager:
        condition: service_started
    restart: unless-stopped
    
# Optional: define multiple network(s)
networks:
  default:
    name: tars-net
    driver: bridge

volumes:
  wake-cache:
    driver: local
  config-db:
    driver: local
